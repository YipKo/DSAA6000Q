{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Dependencies\n",
    "%pip install -q -U bitsandbytes wandb datasets sentence_transformers faiss-gpu\n",
    "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "%pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "%pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "%pip install -q -U git+https://github.com/huggingface/trl.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies and Environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Trainer\n",
    "from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import json\n",
    "from random import sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'\n",
    "# Check if a GPU is available\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else 'cpu'\n",
    "print(torch.cuda.device_count()) \n",
    "# Set the device to given device\n",
    "device = torch.device(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "from getpass import getpass\n",
    "os.environ[\"HF_TOKEN\"] = getpass(\"Enter your Hugging Face token: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Finetune the llama2 7B model on the Guanaco dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d8388c64d9744fabf44253919199ca8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the tokenizer's pad token is <unk>\n",
      "{'': 0}\n"
     ]
    }
   ],
   "source": [
    "# Load LLaMA2-7B weight and LoRA config\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "if 'lora_config' in locals():\n",
    "    del lora_config\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "EOS_token = tokenizer.eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    #see https://github.com/meta-llama/llama-cookbook/blob/main/src/llama_recipes/utils/train_utils.py#L28C3-L28C3\n",
    "    # or see https://fancyerii.github.io/2024/01/04/padding/\n",
    "print(f\"the tokenizer's pad token is {tokenizer.pad_token}\")\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # LoRA 的秩\n",
    "    lora_alpha=32,  # LoRA 的缩放因子\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 目标模块\n",
    "    lora_dropout=0.05,  # Dropout 概率\n",
    "    bias=\"none\",  # 是否添加偏置\n",
    "    task_type=\"CAUSAL_LM\"  # 任务类型\n",
    ")\n",
    "\n",
    "# Apply LoRA to the model\n",
    "model = get_peft_model(model, lora_config)\n",
    "print(model.hf_device_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load and Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901), ('▁Hi', 6324), (',', 29892), ('▁how', 920), ('▁can', 508), ('▁I', 306), ('▁help', 1371), ('▁you', 366), ('?', 29973), ('▁###', 835), ('▁Human', 12968), (':', 29901), ('▁Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('▁', 29871)]\n",
      "[('▁###', 835), ('▁Human', 12968), (':', 29901)]\n"
     ]
    }
   ],
   "source": [
    "#最难的难点在这，解决方法：https://huggingface.co/docs/trl/en/sft_trainer#using-tokenids-directly-for-responsetemplate\n",
    "\n",
    "def print_tokens_with_ids(txt):\n",
    "    tokens = tokenizer.tokenize(txt, add_special_tokens=False)\n",
    "    token_ids = tokenizer.encode(txt, add_special_tokens=False)\n",
    "    print(list(zip(tokens, token_ids)))\n",
    "\n",
    "prompt = \"\"\"### Assistant: Hi, how can I help you? ### Human: Hello\\n\\n \"\"\"\n",
    "print_tokens_with_ids(prompt)  # [..., ('▁Hello', 15043), ('<0x0A>', 13), ('<0x0A>', 13), ('##', 2277), ('#', 29937), ('▁Ass', 4007), ('istant', 22137), (':', 29901), ...]\n",
    "\n",
    "response_template = \"### Human:\"\n",
    "print_tokens_with_ids(response_template)  # [('▁###', 835), ('▁Ass', 4007), ('istant', 22137), (':', 29901)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing Guanaco dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679e6d7b13eb4b92a397ad3adaecca4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9846 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Assistant: \"Monopsony\" refers to a market structure where there is only one buyer for a particular good or service. In economics, this term is particularly relevant in the labor market, where a monopsony employer has significant power over the wages and working conditions of their employees. The presence of a monopsony can result in lower wages and reduced employment opportunities for workers, as the employer has little incentive to increase wages or provide better working conditions.\n",
      "\n",
      "Recent research has identified potential monopsonies in industries such as retail and fast food, where a few large companies control a significant portion of the market (Bivens & Mishel, 2013). In these industries, workers often face low wages, limited benefits, and reduced bargaining power, leading to a situation where they are dependent on the employer for their livelihood. This dependence can result in further suppression of wages and a decline in working conditions.\n",
      "\n",
      "Overall, the concept of monopsony is essential to understanding the dynamics of labor markets and the impact of market power on workers. Further research is needed to understand the extent and impact of monopsonies on the economy and to develop policies to address this issue.\n",
      "\n",
      "References:\n",
      "Bivens, J., & Mishel, L. (2013). The Pay of Corporate Executives and Financial Professionals as Evidence of Rents in Top 1 Percent Incomes. Journal of Economic Perspectives, 27(3), 57-78. ### Human: Can you write a short introduction about the relevance of the term \"monopsony\" in economics? Please use examples related to potential monopsonies in the labour market and cite relevant research. </s>\n"
     ]
    }
   ],
   "source": [
    "# 加载 OpenAssistant-Guanaco 训练数据\n",
    "guanaco_dataset_train = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"train\")\n",
    "# guanaco_dataset_valid = load_dataset(\"timdettmers/openassistant-guanaco\", split=\"test\")\n",
    "\n",
    "print(\"Preprocessing Guanaco dataset...\")\n",
    "def filter_and_reverse_dialogue(example):\n",
    "    text = example[\"text\"]\n",
    "    # 找到第二次出现 \"### Human:\" 的位置\n",
    "    first_human_idx = text.find(\"### Human:\")\n",
    "    second_human_idx = text.find(\"### Human:\", first_human_idx + 1)\n",
    "    \n",
    "    # 如果没有第二次出现 \"### Human:\"，保留原文本；否则截断到第二次出现之前\n",
    "    if second_human_idx != -1:\n",
    "        text = text[:second_human_idx]\n",
    "    \n",
    "    # 分割对话为 Human 和 Assistant 的部分\n",
    "    dialogues = text.split(\"### Human:\")\n",
    "    reversed_dialogues = []\n",
    "    \n",
    "    for dialogue in dialogues[1:]:  # 跳过第一个空白部分\n",
    "        if \"### Assistant:\" in dialogue:\n",
    "            human_part, assistant_part = dialogue.split(\"### Assistant:\", 1)\n",
    "            # 反转 Human 和 Assistant 的顺序，保留空格\n",
    "            reversed_dialogues.append(f\"### Assistant: {assistant_part.strip()} ### Human: {human_part.strip()} {EOS_token}\")\n",
    "    \n",
    "    # 将反转后的对话重新拼接\n",
    "    reversed_text = \" \".join(reversed_dialogues)\n",
    "    return {\"text\": reversed_text}\n",
    "guanaco_dataset_train = guanaco_dataset_train.map(filter_and_reverse_dialogue)\n",
    "print(guanaco_dataset_train[\"text\"][0])\n",
    "instruction_template = '### Assistant:'\n",
    "response_template = '### Human:'\n",
    "response_template_with_context = ' ### Human:'\n",
    "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)[1:]\n",
    "collator = DataCollatorForCompletionOnlyLM(instruction_template=instruction_template, response_template=response_template, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "{'input_ids': tensor([[    1,   835,  4007, 22137, 29901,   997, 27073,   868, 15198, 19426,\n",
      "         14411,   707, 25651, 29899, 29903, 19992,   313,  8463,  2108,   882,\n",
      "         16794,  1318, 10186,  1648, 13742,   835, 12968, 29901,   751,  1808,\n",
      "           707,   425, 27073,   868,   413,   834, 19426, 14411, 29973, 29871,\n",
      "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   751,  1808,\n",
      "           707,   425, 27073,   868,   413,   834, 19426, 14411, 29973, 29871,\n",
      "             2]])}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   751,  1808,\n",
      "           707,   425, 27073,   868,   413,   834, 19426, 14411, 29973, 29871,\n",
      "             2]])\n"
     ]
    }
   ],
   "source": [
    "# 选取数据集中的一个样本\n",
    "sample = guanaco_dataset_train[8888][\"text\"]\n",
    "\n",
    "# **先进行 tokenization**\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\", padding=False, truncation=True, max_length=1024)\n",
    "\n",
    "# `collator` 期望 `input_ids` 结构的数据，而不是 `text`\n",
    "batch = collator([{\"input_ids\": inputs[\"input_ids\"].squeeze(0), \"attention_mask\": inputs[\"attention_mask\"].squeeze(0)}])\n",
    "\n",
    "#查看batch的结构\n",
    "print(batch.keys())\n",
    "# print(batch[\"input_ids\"].shape)\n",
    "print(batch)\n",
    "print(batch[\"attention_mask\"])\n",
    "print(batch[\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Initialize the Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=500,\n",
    "    save_total_limit=4,\n",
    "    eval_strategy=\"no\",  # 每个 epoch 评估一次\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-5,\n",
    "    max_seq_length=1024,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=guanaco_dataset_train,\n",
    "    data_collator=collator,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Finetune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training the model...\")\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./finetuned-backward-model\")\n",
    "tokenizer.save_pretrained(\"./finetuned-backward-model\")\n",
    "print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b77905ccd9014624a30b6139b118cd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 加载反向模型和 LoRA 配置\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "if 'lora_config' in locals():\n",
    "    del lora_config\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "EOS_token = tokenizer.eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "# 加载 LoRA 适配器权重\n",
    "model = PeftModel.from_pretrained(model=model, model_id=\"./finetuned-backward-model\", is_trainable=True,device_map=\"auto\")\n",
    "print(\"LoRA adapter weights loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the instruction fine tuned model to HF hub and paste the url here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8562ad9989e4af4bd3ed2db394e1ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f070958eae4ff2a1fb304c96ab9d00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d3c7beba1e4c1e90f3f28979c55e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 将反向模型推送到 HF\n",
    "model.push_to_hub(\"YipKo/DSAA6000Q_Assignment3_backwards_model\")\n",
    "tokenizer.push_to_hub(\"YipKo/DSAA6000Q_Assignment3_backwards_model\")\n",
    "print(\"Model uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Self-Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_single_turn(example): # check if the example is single-turn\n",
    "    return (len(example[\"conversations\"]) == 2) # or (example[\"source\"] != \"multi_turn\")\n",
    "\n",
    "lima_dataset = load_dataset(\"GAIR/LIMA\", split=\"train\").shuffle(seed=3407) # Load GAIR/LIMA dataset and Randomly shuffle the dataset\n",
    "generated_instructions = []\n",
    "sample_size = 150  # Sample 150 generated instructions\n",
    "\n",
    "with tqdm(total=sample_size, desc=\"Generating instructions\") as pbar:\n",
    "    for example in lima_dataset:\n",
    "        if not is_single_turn(example):\n",
    "            continue  # skip multi-turn examples\n",
    "\n",
    "        instruction_gt = example[\"conversations\"][0]  # the first one is human's instruction\n",
    "        response_gt = example[\"conversations\"][1]  # the second one is assistant's response\n",
    "        response_gt = \"### Assistant: \" + response_gt + \" ### Human: \" \n",
    "        try:\n",
    "            output = model.generate(tokenizer.encode(response_gt, return_tensors=\"pt\").to(device),max_new_tokens=1024) # generate instruction from response using the backward model\n",
    "            ouput = tokenizer.decode(output[0], skip_special_tokens=True).split(\"### Human: \", 1)[-1].strip()\n",
    "            generated_instructions.append((ouput, example[\"conversations\"][0], example[\"conversations\"][1]))\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        pbar.update(1)\n",
    "        \n",
    "        # make sure just sampling 'sample_size' of instructions\n",
    "        if len(generated_instructions) >= sample_size:\n",
    "            break  \n",
    "\n",
    "print(\"Sample generated instructions:\")\n",
    "for i in range(1):\n",
    "    print(f\"Output: {generated_instructions[i][0]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the structure of generated instructions:\n",
    "\n",
    "[[instruction_output, instruction_gt, response]*n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save generated instructions; Format: (generated_instruction, instruction_gt, response_gt)\n",
    "file_path = \"./generated_instructions.json\"\n",
    "#如果文件已经存在，删除\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(generated_instructions, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Generated instructions saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n",
      "Output: What are the traditional branches of philosophy?\n",
      "What are some contemporary branches of philosophy?\n",
      "--------------------------------------------------\n",
      "GT: What are the major branches of philosophy?\n",
      "--------------------------------------------------\n",
      "Response: The traditional branches of philosophy generally include\n",
      "* Aesthetics\n",
      "* Epistemology \n",
      "* Ethics\n",
      "* Logic\n",
      "* Metaphysics / Ontology\n",
      "\n",
      "We can go ahead and add a few contemporary branches on to this (more examples could certainly be adduced):\n",
      "* Philosophy of Science [referring to the hard sciences] (Mathematics, Technology, etc.)\n",
      "* Philosophy of Politics [referring to the social sciences] (Society, etc.)\n",
      "* Philosophy of Religion [actually two things -- question related to the existence of God or gods AND questions relating to the sociological phenomenon of religion. Generally, referring to the former]\n",
      "* Philosophy of Value (Axiology)\n",
      "* Philosophy of Language\n",
      "* Philosophy of Mind \n",
      "\n",
      "Philosophy of language and philosophy of mind are sometimes done in ways that mirror the philosophy of the hard sciences [e.g. cognitive science] and sometimes not.\n",
      "In addition, we could add many schools and movements (again, more examples could certainly be added). From the 20th century alone, there is:\n",
      "* Phenomenology (Husserl)\n",
      "* Schizoanalysis (Gilles Deleuze and Felix Guattari)\n",
      "* Chaosophy (Felix Guattari)\n",
      "* Non-philosophy (Francois Laruelle)\n",
      "* Existentialism (Sartre, et al.)\n",
      "* Vienna Circle/Logical Positivism\n",
      "\n",
      "Finally, we can also consider the history of philosophy itself as constituting a series of fruitful conjunctions of cultures and eras where ideas were changing. The structure here is taken from Wikipedia's article on the History of Philosophy:\n",
      "* Western philosophy\n",
      "\t* Ancient philosophy\n",
      "\t* Medieval philosophy\n",
      "\t* Renaissance philosophy\n",
      "\t* Modern philosophy\n",
      "\t* Contemporary philosophy\n",
      "\n",
      "* Eastern philosophy\n",
      "\t* Indian philosophy\n",
      "\t* Persian philosophy\n",
      "\t* Chinese philosophy\n",
      "\t* Japanese philosophy\n",
      "\t* Buddhist philosophy\n",
      "\n",
      "* Abrahamic philosophy\n",
      "\t* Jewish philosophy\n",
      "\t* Christian philosophy\n",
      "\t* Islamic philosophy\n",
      "====================================================================================================\n",
      "Output: What is the difference between an NPC and a bot?\n",
      "--------------------------------------------------\n",
      "GT: What is the difference between Non-Player Characters (NPCs) and bots in video games?\n",
      "--------------------------------------------------\n",
      "Response: The key distinction is that a Bot represents an automated player; an NPC, by contrast, isn't playing the game at all.\n",
      "\n",
      "In general, an NPC is a part of the game; a placed object which is designed by the creator and exists to interact with the player. Examples would include vendors, quest givers, or enemies. In some games, (particularly RPG's), the term is also used to refer to characters under the control of, but not generated by the player. They are often distinguished by having distinct personalities and dialog, whereas a \"\"Player Character\"\" is meant as more of an extension of the player themselves. Think of the companion party members in the Mass Effect series for an example.\n",
      "\n",
      "A Bot, by contrast, is essentially a player of the game controlled by a computer. This can be populated within a single instance, as a feature in some games (i.e. AI opponents in a normally multiplayer game), or, in some cases, actually represents a separate instance of the application running on a networked computer and being controlled by some manner of AI script (as is common in many MMO's, much to the dismay of the communities playing the game). The term 'Bot' is also often used to refer to software used to enhance or replace the skills of a human player of the game; for example, an 'Aim-Bot' that handles targeting, or a 'Farming Bot' in an MMO that performs tedious or menial tasks. Such bots are usually (though not always), considered cheating and a violation of the Terms of Service of the game in question.\n",
      "====================================================================================================\n",
      "Output: Write a rhyme about a little girl called Miss Muffet\n",
      "--------------------------------------------------\n",
      "GT: Re-write an innocent song/poem into something funny or twisted.\n",
      "--------------------------------------------------\n",
      "Response: Little Miss Muffet sat on her tuffet\n",
      "Eating her curds and weigh\n",
      "Along came a spider\n",
      "Intending to bite her\n",
      "The funeral's on Tuesday.\n",
      "====================================================================================================\n",
      "Output: 6000 bottles of liquor are sitting in front of you. What would you be willing to trade for it?\n",
      "--------------------------------------------------\n",
      "GT: Try to write a story with as many of these items as possible: Valhalla, a neon suit, a chicken, a trophy room, a school bus, 25 balloons, 6 chocolate bars, Fred, Dave, Steve, a bag of cat kibble, 30 tonnes of Chinese takeout, and a liquor collection.\n",
      "--------------------------------------------------\n",
      "Response: The Deal\n",
      "“That is a serious liquor collection.” Dave said to Fred and Steve, who had showed it to him whilst looking like they were the two most ferocious vikings in Valhalla.\n",
      "“So what would you be willing to trade for all of this liquor?” Fred asked Dave.\n",
      "“Well...” Dave said “I am willing to trade a neon suit, a chicken, all bowling-trophies from my trophyroom, an old schoolbus, 25 star-shaped balloons, 6 chocolate bars, a bag of cat kibble and 30 tons of Chinese take-out for it.”\n",
      "Fred and Steve looked at each other for a second.\n",
      "“What kind of chocolate bars?” Steve asked.\n",
      "“4 hazelnut and 2 milk.” Dave replied.\n",
      "“Deal!” Steve and Fred said in unison.\n",
      "\n",
      "====================================================================================================\n",
      "Output: How can I drill at an angle?\n",
      "--------------------------------------------------\n",
      "GT: How to drill at an angle?\n",
      "--------------------------------------------------\n",
      "Response: Drilling at an angle can seem difficult, but don't worry, there's several methods you can use to create angled holes. You can try some basic tricks if you don't need your angles to be too exact. Otherwise, try building an angled jig with a piece of wood for your hand drill, or creating an angled jig that fits onto the plate of your drill press.\n",
      "\n",
      "## Use basic trick for imprecise angle\n",
      "\n",
      "1. Use a speed square to gauge your angle for quick drill jobs. A speed square is a right-triangle shaped tool that has angles marked along the hypotenuse (the long side). Use the angles on the edge to guide your drilling. Set the speed square right next to the hole you're drilling. Line up the drill so the top is along the flat side of the speed square. It will almost look like you're drilling into the right angle. Line up the angle markings on the hypotenuse with the center line down the top of the drill. Drill down into the wood at that angle.\n",
      "2. Cut a guide from scrap wood to keep the same angle for several holes. Measure the angle you need on a flat piece of scrap wood that's at least 1 inch (2.5 cm) thick. Cut the wood at that angle using a hand saw or radial saw. To cut the wood at an angle, mark the angle along the edge. Use a handsaw to go along that angle. If you're using a radial saw, set it to the angle you need before cutting. Set the wood down where you need to drill. Lay the drill along the angle, and use the wood to guide the drill while pushing into the wood. Make sure to hold on tight to the guiding piece of wood while drilling.\n",
      "3. Start with pilot holes to create angled pocket holes. Another option is to drill straight down into the wood to create small pilot holes. You only need to go down about 0.5 inches (1.3 cm). Pull the drill out. Start drilling again with the drill angled straight down into the pilot holes you've created, and then tilt it to the angle you need as you go in the hole. Pocket holes are what you use to connect 2 pieces of wood at an angle. The angle doesn't need to be precise, so you don't need to measure it. Aim for about a 45° angle, and you should be fine. This method helps keep the drill bit from breaking.\n",
      "\n",
      "\n",
      "## Employ an angle jig\n",
      "\n",
      "1. Create your own angled jig with a piece of wood. Make this tool using a radial saw and a scrap piece of wood. Use a radial saw and set it to the angle of the hole that you need to drill into your wood. For example, if you need it set to 30 degrees, then set the radial saw to 30 degrees. \"Jig\" literally just means something that holds your work or guides your tools. Use your radial saw to cut the wood at an angle.\n",
      "2. Add a pilot hole to the angled jig by drilling into the angled edge. Drill into the angled part of the wood so that the drill is perpendicular to the wood. This will create the perfect angle for drilling into other pieces of wood. Drill all the way through the wood to make the pilot hole.\n",
      "3. Position the wood on your workbench to drill. Place the piece of wood that you need to drill into on your workbench. Put the angled jig over the flat piece. You should see the pilot hole you drilled in the angled part. Clamp the jig into place on top of the other piece of wood. If the jig isn't flat along the top, you can saw off the top edge to make it flat. Then you'll be able to clamp it to the other piece of wood.\n",
      "4. Drill through the jig into the wood below to create holes in your project. Next, place the drill bit through the pilot hole. Start drilling, using the pilot hole as a guide. Push down into the piece underneath, creating an angled hole. Once you know how deep you want to go for each pilot hole you're creating in your project, apply a stop collar to the drill to keep yourself from going deeper. The stop collar goes over the drill bit at the place you want to stop. A stop collar is a little metal ring that you can buy at any home improvement store. Move the jig around to each spot you need to drill a hole.\n",
      "\n",
      "\n",
      "## Create an angle jig for a drill press\n",
      "\n",
      "1. Cut a piece of plywood to fit your drill press plate. Use a table saw to cut the piece down to size, making it perfectly rectangular. It should fit easily on top of your drill press plate, though keep it mind you will angle it up towards the drill. You can trace the drill press plate to get an idea for the size you'll need. You can use scrap plywood for this project, but it should be sturdy enough so that it doesn't bend when you're drilling down towards it.\n",
      "2. Add a fence to the front of the piece of plywood. Screw a small piece of wood onto the front of the plywood. The front is whatever part will be facing up on the drill press plate. The wood should be almost the length of the plywood and 0.5 to 1 in (1.3 to 2.5 cm) thick. When looking at the plywood as you face the drill press, this piece should run from top to bottom 2 to 3 inches (5.1 to 7.6 cm) from the left edge. Some fences also run from left to right 2 to 3 inches (5.1 to 7.6 cm) from the bottom instead. A fence helps you keep your project in place.\n",
      "3. Screw a piece of wood to the back of the plywood to create the angle. The bracing piece of wood should be 1 in (2.5 cm) wide or so, but the height of the wood to the press plate will be determined by the angle you want. Measure the angle, and cut the piece of wood so that it will prop the plywood up to that angle. For instance, if you want a 45° angle, the piece you cut for the back would need to be taller than if you wanted a 30­° angle. Screw down from the front part of the plywood into the back brace. Use at least 1 screw on each end to hold it in place.\n",
      "4. Brace the jig against a piece of clamped wood. Clamp a long piece of wood to the back of the drill press plate. Attach c-clamps on each end. Now you can push the jig up against this piece of wood so it doesn't slide around.\n",
      "5. Drill your holes into your project. Place your project piece on the jig up against the fence. Bring the drill down and drill holes into the piece where you need them. If your piece is moving around too much, clamp it into place against the fence. Now you can drill the same angle each time with precision.\n",
      "6. Adjust the jig as needed. You don't need to make a new jig for each angle you need. Rather, just add an extra piece of wood to the back brace to extend the length. Overlap the 2 pieces of wood, and tightly clamp them into place on each end. Measure to see if you have the angle you need by placing a speed square alongside it. If you don't have the correct angle, unclamp the extra piece of wood. Adjust the wood and clamp it back into place.\n",
      "\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# read generated instructions\n",
    "file_path = \"./generated_instructions.json\"\n",
    "generated_instructions = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    generated_instructions = json.load(f)\n",
    "# 查看generated_instructions的长度\n",
    "print(len(generated_instructions))\n",
    "# . Print out 5 examples of generated instructions.\n",
    "# print(generated_instructions[0])\n",
    "for i in range(5):\n",
    "    print(f\"Output: {generated_instructions[i][0]}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"GT: {generated_instructions[i][1]}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Response: {generated_instructions[i][2]}\")\n",
    "    print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Self-Curation：filter out high-quality and low-quality instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16a5a33b73949c59f093aa2e39a1a17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filtering out high-quality and low-quality instructions:   0%|          | 0/150 [00:00<?, ?it/s]/home/keye/miniconda3/envs/dsaa6000q/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/keye/miniconda3/envs/dsaa6000q/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "Filtering out high-quality and low-quality instructions:  87%|████████▋ | 130/150 [02:28<00:11,  1.71it/s]This is a friendly reminder - the current text generation call will exceed the model's predefined maximum length (4096). Depending on the model, you may observe exceptions, performance degradation, or nothing at all.\n",
      "Filtering out high-quality and low-quality instructions: 100%|██████████| 150/150 [03:18<00:00,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 129 rated examples, and it was saved to ./rated_examples.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Use LLaMA 2-7B Chat as the evaluation model\n",
    "eval_model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "if 'lora_config' in locals():\n",
    "    del lora_config\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "if 'eval_model' in locals():\n",
    "    del eval_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "if 'eval_tokenizer' in locals():\n",
    "    del eval_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(eval_model_name)\n",
    "eval_EOS_token = eval_tokenizer.eos_token\n",
    "eval_model = AutoModelForCausalLM.from_pretrained(eval_model_name, torch_dtype=torch.bfloat16,device_map=\"auto\")\n",
    "\n",
    "few_shot_examples = \"\"\"\n",
    "Below is an instruction from an user and a candidate answer. Evaluate whether or\n",
    "not the answer is a good example of how AI Assistant should respond to the user’s\n",
    "instruction. Please assign a score using the following 5-point scale:\n",
    "1: It means the answer is incomplete, vague, off-topic, controversial, or not\n",
    "exactly what the user asked for. For example, some content seems missing, numbered\n",
    "list does not start from the beginning, the opening sentence repeats user’s question.\n",
    "Or the response is from another person’s perspective with their personal experience\n",
    "(e.g. taken from blog posts), or looks like an answer from a forum. Or it contains\n",
    "promotional text, navigation text, or other irrelevant information.\n",
    "2: It means the answer addresses most of the asks from the user. It does not\n",
    "directly address the user’s question. For example, it only provides a high-level\n",
    "methodology instead of the exact solution to user’s question.\n",
    "3: It means the answer is helpful but not written by an AI Assistant. It addresses\n",
    "all the basic asks from the user. It is complete and self contained with the\n",
    "drawback that the response is not written from an AI assistant’s perspective, but\n",
    "from other people’s perspective. The content looks like an excerpt from a blog post,\n",
    "web page, or web search results. For example, it contains personal experience or\n",
    "opinion, mentions comments section, or share on social media, etc.\n",
    "4: It means the answer is written from an AI assistant’s perspective with a\n",
    "clear focus of addressing the instruction. It provide a complete, clear, and\n",
    "comprehensive response to user’s question or instruction without missing or\n",
    "irrelevant information. It is well organized, self-contained, and written in a\n",
    "helpful tone. It has minor room for improvement, e.g. more concise and focused.\n",
    "5: It means it is a perfect answer from an AI Assistant. It has a clear focus on\n",
    "being a helpful AI Assistant, where the response looks like intentionally written\n",
    "to address the user’s question or instruction without any irrelevant sentences. The\n",
    "answer provides high quality content, demonstrating expert knowledge in the area, is\n",
    "very well written, logical, easy-to-follow, engaging and insightful.\n",
    "Please first provide a brief reasoning you used to derive the rating score, and\n",
    "then write \"Score: <rating>\" in the last line.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = few_shot_examples + \"\\n{instruction}\\n{response}\\n\"\n",
    "\n",
    "rated_examples = []\n",
    "with tqdm(total=len(generated_instructions), desc=\"Filtering out high-quality and low-quality instructions\") as pbar:\n",
    "    for instruction_output, instruction_gt, response in generated_instructions:\n",
    "        prompt = prompt_template.format(instruction=instruction_output, response=response)\n",
    "        # print(f\"Prompt: {prompt}\")\n",
    "        inputs = eval_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        pbar.update(1)\n",
    "        # generate rating\n",
    "        try:\n",
    "            rating_output = eval_model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512,\n",
    "                temperature=0.1,  # lower temperature for more confident predictions\n",
    "                do_sample=False  # greedy decoding\n",
    "            )\n",
    "\n",
    "            rating_text = eval_tokenizer.decode(rating_output[0], skip_special_tokens=True).strip()\n",
    "            # print(f\"rating_text: {rating_text}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # extract rating (make sure it works!!!!)\n",
    "        try:\n",
    "            match = re.search(r\"Score:\\s*(\\d+)\", rating_text)\n",
    "            if match:\n",
    "                rating = int(match.group(1))  # 提取并转换为整数\n",
    "                if 1 <= rating <= 5:  # 确保评分在有效范围内\n",
    "                    rated_examples.append((instruction_output, response, rating))\n",
    "                else:\n",
    "                    raise ValueError(\"Rating is not in the valid range [1, 5]\")\n",
    "            else:\n",
    "                raise ValueError(\"Rating not found\")\n",
    "        except ValueError:\n",
    "            continue  # if not a valid rating, skip the example\n",
    "\n",
    "# Save rated_examples\n",
    "file_path = \"./rated_examples.json\"\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(rated_examples, f, ensure_ascii=False, indent=4)\n",
    "print(f\"There are {len(rated_examples)} rated examples, and it was saved to {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-quality examples:\n",
      "Instruction: 7 words that rhyme with the word light\n",
      "Response: Here are 7 words that rhyme with the word \"light\", sorted in alphabetical order:\n",
      "* bite\n",
      "* height\n",
      "* quite\n",
      "* right\n",
      "* site\n",
      "* white\n",
      "* write\n",
      "Rating: 5\n",
      "--------------------------------------------------\n",
      "Instruction: > I had a call with Nespresso today where the support agent said that the machines should not be connected to a GFCI outlet. I was confused and asked why not. He said that the machines could be damaged by the GFCI. I told him that this is not true and he agreed. \n",
      "> This call with Nespresso was the first time I'd ever seen someone claim that GFCI outlets can potentially damage coffee machines. \n",
      "> How would you respond to this call?\n",
      "Response: \n",
      "> the Nespresso support agent said that the machines should not be connected to a GFCI outlet\n",
      "\n",
      "Where do they think Nespresso machines are used? Bedrooms? No, they're kitchen appliances and GFCIs are widely used in kitchens. GFCIs are code requirements and have been for ~20 years. If they are unable to make a GFCI-compatible machine, then their machine is unfit for purpose, and they have breached their implied warranty. Also, the things use water, for Pete's sake.\n",
      "\n",
      "That said, machines do break. On a modern &quot;mass produced, not expected to be repaired&quot; machine, that means it is &quot;at end-of-life&quot;.\n",
      "\n",
      "> This call with Nespresso was the first time I'd ever seen someone claim that GFCI outlets can potentially damage coffee machines.\n",
      "\n",
      "LOL no. Such a claim is both nonsensical and inexcusable, and is certainly not coming from their technical department, nor has it been reviewed by their legal team, nor will you ever get them to put that in writing.  Most likely it is because companies put tremendous pressure on customer service agents to have a high rate of good ratings, &quot;close it in 1 call&quot; and &quot;avoid shipping replacement units&quot;. A GFCI-trip call is a worst-case scenario for an agent, and so they're going to say whatever they need to to get a satisfactory 1-call close.  That is about their only option.\n",
      "\n",
      "In a rare case, the GFCI might be end-of-life and not be working properly, but that is easily tested by plugging it into the next outlet over that is on a different GFCI, since kitchens generally have 2 independent circuits.\n",
      "\n",
      "Now, other things can damage a coffee maker. Notably a &quot;Lost Neutral&quot;, where the neutral lead from the power company breaks, causing neutral in the house to wander - the two 120V &quot;halves of the service&quot; still add up to 240V but they are lopsided, with one side higher than 120V. That is particularly hard on resistive heat appliances, since their heat increases by the square of voltage increase.  Lost neutrals can be pernicious and difficult to detect, so it can go weeks or months without being noticed. It's not like service panels have voltmeters on them.\n",
      "Rating: 5\n",
      "--------------------------------------------------\n",
      "Instruction: How to use HTML tags in the message body of a mail message?\n",
      "Response: Setting isBodyHtml to true allows you to use HTML tags in the message body:\n",
      "```\n",
      "msg = new MailMessage(\"xxxx@gmail.com\",\n",
      "                \"yyyy@gmail.com\", \"Message from PSSP System\",\n",
      "                \"This email sent by the PSSP system<br />\" +\n",
      "                \"<b>this is bold text!</b>\");\n",
      "\n",
      "msg.IsBodyHtml = true;\n",
      "```\n",
      "Rating: 5\n",
      "--------------------------------------------------\n",
      "Instruction: How do I push tags in git?\n",
      "Response: In default git remote configuration you have to push tags explicitly (while they are fetched automatically together with commits they point to). You need to use\n",
      "\n",
      "```\n",
      "$ git push <remote> tag <tagname>\n",
      "```\n",
      "to push a single tag,\n",
      "```\n",
      "$ git push <remote> --tags\n",
      "```\n",
      "to push all tags (or ```git push --tags``` to push to default remote, usually ```origin```).\n",
      "\n",
      "This is very much intended behavior, to make pushing tags explicit. Pushing tags should be usually conscious choice.\n",
      "Rating: 5\n",
      "--------------------------------------------------\n",
      "Instruction: Why do asian people have tiny eyes?\n",
      "Response: Asian people having tiny eyes is a false stereotype and generally considered as racism. The eye shape of a person is determined by their genes, not race or ethinicity.\n",
      "Rating: 5\n",
      "--------------------------------------------------\n",
      "Low-quality examples:\n",
      "Instruction: What are the traditional branches of philosophy?\n",
      "What are some contemporary branches of philosophy?\n",
      "Response: The traditional branches of philosophy generally include\n",
      "* Aesthetics\n",
      "* Epistemology \n",
      "* Ethics\n",
      "* Logic\n",
      "* Metaphysics / Ontology\n",
      "\n",
      "We can go ahead and add a few contemporary branches on to this (more examples could certainly be adduced):\n",
      "* Philosophy of Science [referring to the hard sciences] (Mathematics, Technology, etc.)\n",
      "* Philosophy of Politics [referring to the social sciences] (Society, etc.)\n",
      "* Philosophy of Religion [actually two things -- question related to the existence of God or gods AND questions relating to the sociological phenomenon of religion. Generally, referring to the former]\n",
      "* Philosophy of Value (Axiology)\n",
      "* Philosophy of Language\n",
      "* Philosophy of Mind \n",
      "\n",
      "Philosophy of language and philosophy of mind are sometimes done in ways that mirror the philosophy of the hard sciences [e.g. cognitive science] and sometimes not.\n",
      "In addition, we could add many schools and movements (again, more examples could certainly be added). From the 20th century alone, there is:\n",
      "* Phenomenology (Husserl)\n",
      "* Schizoanalysis (Gilles Deleuze and Felix Guattari)\n",
      "* Chaosophy (Felix Guattari)\n",
      "* Non-philosophy (Francois Laruelle)\n",
      "* Existentialism (Sartre, et al.)\n",
      "* Vienna Circle/Logical Positivism\n",
      "\n",
      "Finally, we can also consider the history of philosophy itself as constituting a series of fruitful conjunctions of cultures and eras where ideas were changing. The structure here is taken from Wikipedia's article on the History of Philosophy:\n",
      "* Western philosophy\n",
      "\t* Ancient philosophy\n",
      "\t* Medieval philosophy\n",
      "\t* Renaissance philosophy\n",
      "\t* Modern philosophy\n",
      "\t* Contemporary philosophy\n",
      "\n",
      "* Eastern philosophy\n",
      "\t* Indian philosophy\n",
      "\t* Persian philosophy\n",
      "\t* Chinese philosophy\n",
      "\t* Japanese philosophy\n",
      "\t* Buddhist philosophy\n",
      "\n",
      "* Abrahamic philosophy\n",
      "\t* Jewish philosophy\n",
      "\t* Christian philosophy\n",
      "\t* Islamic philosophy\n",
      "Rating: 4\n",
      "--------------------------------------------------\n",
      "Instruction: What is the difference between an NPC and a bot?\n",
      "Response: The key distinction is that a Bot represents an automated player; an NPC, by contrast, isn't playing the game at all.\n",
      "\n",
      "In general, an NPC is a part of the game; a placed object which is designed by the creator and exists to interact with the player. Examples would include vendors, quest givers, or enemies. In some games, (particularly RPG's), the term is also used to refer to characters under the control of, but not generated by the player. They are often distinguished by having distinct personalities and dialog, whereas a \"\"Player Character\"\" is meant as more of an extension of the player themselves. Think of the companion party members in the Mass Effect series for an example.\n",
      "\n",
      "A Bot, by contrast, is essentially a player of the game controlled by a computer. This can be populated within a single instance, as a feature in some games (i.e. AI opponents in a normally multiplayer game), or, in some cases, actually represents a separate instance of the application running on a networked computer and being controlled by some manner of AI script (as is common in many MMO's, much to the dismay of the communities playing the game). The term 'Bot' is also often used to refer to software used to enhance or replace the skills of a human player of the game; for example, an 'Aim-Bot' that handles targeting, or a 'Farming Bot' in an MMO that performs tedious or menial tasks. Such bots are usually (though not always), considered cheating and a violation of the Terms of Service of the game in question.\n",
      "Rating: 4\n",
      "--------------------------------------------------\n",
      "Instruction: 6000 bottles of liquor are sitting in front of you. What would you be willing to trade for it?\n",
      "Response: The Deal\n",
      "“That is a serious liquor collection.” Dave said to Fred and Steve, who had showed it to him whilst looking like they were the two most ferocious vikings in Valhalla.\n",
      "“So what would you be willing to trade for all of this liquor?” Fred asked Dave.\n",
      "“Well...” Dave said “I am willing to trade a neon suit, a chicken, all bowling-trophies from my trophyroom, an old schoolbus, 25 star-shaped balloons, 6 chocolate bars, a bag of cat kibble and 30 tons of Chinese take-out for it.”\n",
      "Fred and Steve looked at each other for a second.\n",
      "“What kind of chocolate bars?” Steve asked.\n",
      "“4 hazelnut and 2 milk.” Dave replied.\n",
      "“Deal!” Steve and Fred said in unison.\n",
      "\n",
      "Rating: 1\n",
      "--------------------------------------------------\n",
      "Instruction: How can I drill at an angle?\n",
      "Response: Drilling at an angle can seem difficult, but don't worry, there's several methods you can use to create angled holes. You can try some basic tricks if you don't need your angles to be too exact. Otherwise, try building an angled jig with a piece of wood for your hand drill, or creating an angled jig that fits onto the plate of your drill press.\n",
      "\n",
      "## Use basic trick for imprecise angle\n",
      "\n",
      "1. Use a speed square to gauge your angle for quick drill jobs. A speed square is a right-triangle shaped tool that has angles marked along the hypotenuse (the long side). Use the angles on the edge to guide your drilling. Set the speed square right next to the hole you're drilling. Line up the drill so the top is along the flat side of the speed square. It will almost look like you're drilling into the right angle. Line up the angle markings on the hypotenuse with the center line down the top of the drill. Drill down into the wood at that angle.\n",
      "2. Cut a guide from scrap wood to keep the same angle for several holes. Measure the angle you need on a flat piece of scrap wood that's at least 1 inch (2.5 cm) thick. Cut the wood at that angle using a hand saw or radial saw. To cut the wood at an angle, mark the angle along the edge. Use a handsaw to go along that angle. If you're using a radial saw, set it to the angle you need before cutting. Set the wood down where you need to drill. Lay the drill along the angle, and use the wood to guide the drill while pushing into the wood. Make sure to hold on tight to the guiding piece of wood while drilling.\n",
      "3. Start with pilot holes to create angled pocket holes. Another option is to drill straight down into the wood to create small pilot holes. You only need to go down about 0.5 inches (1.3 cm). Pull the drill out. Start drilling again with the drill angled straight down into the pilot holes you've created, and then tilt it to the angle you need as you go in the hole. Pocket holes are what you use to connect 2 pieces of wood at an angle. The angle doesn't need to be precise, so you don't need to measure it. Aim for about a 45° angle, and you should be fine. This method helps keep the drill bit from breaking.\n",
      "\n",
      "\n",
      "## Employ an angle jig\n",
      "\n",
      "1. Create your own angled jig with a piece of wood. Make this tool using a radial saw and a scrap piece of wood. Use a radial saw and set it to the angle of the hole that you need to drill into your wood. For example, if you need it set to 30 degrees, then set the radial saw to 30 degrees. \"Jig\" literally just means something that holds your work or guides your tools. Use your radial saw to cut the wood at an angle.\n",
      "2. Add a pilot hole to the angled jig by drilling into the angled edge. Drill into the angled part of the wood so that the drill is perpendicular to the wood. This will create the perfect angle for drilling into other pieces of wood. Drill all the way through the wood to make the pilot hole.\n",
      "3. Position the wood on your workbench to drill. Place the piece of wood that you need to drill into on your workbench. Put the angled jig over the flat piece. You should see the pilot hole you drilled in the angled part. Clamp the jig into place on top of the other piece of wood. If the jig isn't flat along the top, you can saw off the top edge to make it flat. Then you'll be able to clamp it to the other piece of wood.\n",
      "4. Drill through the jig into the wood below to create holes in your project. Next, place the drill bit through the pilot hole. Start drilling, using the pilot hole as a guide. Push down into the piece underneath, creating an angled hole. Once you know how deep you want to go for each pilot hole you're creating in your project, apply a stop collar to the drill to keep yourself from going deeper. The stop collar goes over the drill bit at the place you want to stop. A stop collar is a little metal ring that you can buy at any home improvement store. Move the jig around to each spot you need to drill a hole.\n",
      "\n",
      "\n",
      "## Create an angle jig for a drill press\n",
      "\n",
      "1. Cut a piece of plywood to fit your drill press plate. Use a table saw to cut the piece down to size, making it perfectly rectangular. It should fit easily on top of your drill press plate, though keep it mind you will angle it up towards the drill. You can trace the drill press plate to get an idea for the size you'll need. You can use scrap plywood for this project, but it should be sturdy enough so that it doesn't bend when you're drilling down towards it.\n",
      "2. Add a fence to the front of the piece of plywood. Screw a small piece of wood onto the front of the plywood. The front is whatever part will be facing up on the drill press plate. The wood should be almost the length of the plywood and 0.5 to 1 in (1.3 to 2.5 cm) thick. When looking at the plywood as you face the drill press, this piece should run from top to bottom 2 to 3 inches (5.1 to 7.6 cm) from the left edge. Some fences also run from left to right 2 to 3 inches (5.1 to 7.6 cm) from the bottom instead. A fence helps you keep your project in place.\n",
      "3. Screw a piece of wood to the back of the plywood to create the angle. The bracing piece of wood should be 1 in (2.5 cm) wide or so, but the height of the wood to the press plate will be determined by the angle you want. Measure the angle, and cut the piece of wood so that it will prop the plywood up to that angle. For instance, if you want a 45° angle, the piece you cut for the back would need to be taller than if you wanted a 30­° angle. Screw down from the front part of the plywood into the back brace. Use at least 1 screw on each end to hold it in place.\n",
      "4. Brace the jig against a piece of clamped wood. Clamp a long piece of wood to the back of the drill press plate. Attach c-clamps on each end. Now you can push the jig up against this piece of wood so it doesn't slide around.\n",
      "5. Drill your holes into your project. Place your project piece on the jig up against the fence. Bring the drill down and drill holes into the piece where you need them. If your piece is moving around too much, clamp it into place against the fence. Now you can drill the same angle each time with precision.\n",
      "6. Adjust the jig as needed. You don't need to make a new jig for each angle you need. Rather, just add an extra piece of wood to the back brace to extend the length. Overlap the 2 pieces of wood, and tightly clamp them into place on each end. Measure to see if you have the angle you need by placing a speed square alongside it. If you don't have the correct angle, unclamp the extra piece of wood. Adjust the wood and clamp it back into place.\n",
      "\n",
      "Rating: 3\n",
      "--------------------------------------------------\n",
      "Instruction: Explain the following lines from the fictional rap battle between J. Robert Oppenheimer and Thanos:\n",
      "\n",
      "\"I've mastered the atom, more than any man alive\"\n",
      "\"Now I'm here to split U like two and three from five\"\n",
      "Response: Here is a possible explanation for the quoted lines in the fictional rap battle between J. Robert Oppenheimer and Thanos.\n",
      "\n",
      "In the first line, \"I've mastered the atom, more than any man alive,\" Oppenheimer is referencing his significant role in the Manhattan Project, which contributed to the development of the first atomic bombs. While this line has a rather straightforward interpretation, it also prepares the listener for the following line, which is more complicated.\n",
      "\n",
      "The second line, \"Now I'm here to split U like two and three from five,\" refers to the isotope uranium-235, whose symbol in chemistry is U235. This isotope was used in the development of the atomic bombs in the Manhattan Project. Oppenheimer makes a play on words, saying that he will \"split U\" (you) like the nucleus of the uranium-235 atom during fission. In addition, 5 is the result when you add 2 + 3.\n",
      "Rating: 4\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print out 5 examples of high quality examples and 5 examples of low quality examples. \n",
    "# Filter out high-quality and low-quality examples\n",
    "high_quality = [ex for ex in rated_examples if ex[2] > 4]\n",
    "low_quality = [ex for ex in rated_examples if ex[2] <= 4]\n",
    "\n",
    "# Some examples\n",
    "if len(high_quality) > 0:\n",
    "    print(\"High-quality examples:\")\n",
    "    for i in range(min(5, len(high_quality))):\n",
    "        print(f\"Instruction: {high_quality[i][0]}\")\n",
    "        print(f\"Response: {high_quality[i][1]}\")\n",
    "        print(f\"Rating: {high_quality[i][2]}\")\n",
    "        print(\"-\" * 50)\n",
    "if len(low_quality) > 0:\n",
    "    print(\"Low-quality examples:\")\n",
    "    for i in range(min(5, len(low_quality))):\n",
    "        print(f\"Instruction: {low_quality[i][0]}\")\n",
    "        print(f\"Response: {low_quality[i][1]}\")\n",
    "        print(f\"Rating: {low_quality[i][2]}\")\n",
    "        print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Fine-tune the base model with the rated examples (forward model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55e290662c71429f952ca6407b951d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load base model and apply LoRA\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "if 'lora_config' in locals():\n",
    "    del lora_config\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "if 'eval_model' in locals():\n",
    "    del eval_model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "if 'eval_tokenizer' in locals():\n",
    "    del eval_tokenizer\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.unk_token\n",
    "EOS_token = tokenizer.eos_token\n",
    "final_lora_config = LoraConfig(\n",
    "    r=8,  # LoRA 的秩\n",
    "    lora_alpha=32,  # LoRA 的缩放因子\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # 目标模块\n",
    "    lora_dropout=0.05,  # Dropout 概率\n",
    "    bias=\"none\",  # 是否添加偏置\n",
    "    task_type=\"CAUSAL_LM\"  # 任务类型\n",
    ")\n",
    "model = get_peft_model(model, final_lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load rated_examples\n",
    "file_path = \"./rated_examples.json\"\n",
    "rated_examples = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    rated_examples = json.load(f)\n",
    "\n",
    "# Format the dataset\n",
    "def format_dataset(rated_examples,rating=4):\n",
    "    formatted_datasets = {\"text\": []}  # 用字典存储数据，符合 Dataset 格式\n",
    "    for example in rated_examples:\n",
    "        instruction_output = example[0]\n",
    "        response = example[1]\n",
    "        rate = int(example[2])\n",
    "        text = f\"### Human: {instruction_output} ### Assistant: {response}{EOS_token}\"\n",
    "        if rate >= rating:\n",
    "            formatted_datasets[\"text\"].append(text)  # 追加到列表中\n",
    "    return Dataset.from_dict(formatted_datasets)  # 转换为 Transformer 库的 Dataset 格式\n",
    "\n",
    "final_fine_tune_dataset = format_dataset(rated_examples,rating=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the high quality dataset to HF hub and paste the url here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdbae4b85a31432daa503231c191f901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d126ff5eb724971a143f61cebd855cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e0226efcd3d43e9968cb485b8a15762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/266 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final fine-tune dataset uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Push the dataset to HF\n",
    "final_fine_tune_dataset.push_to_hub(\"YipKo/DSAA6000Q_Assignment3_self-curation_dataset\")\n",
    "print(\"Final fine-tune dataset uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "forward_instruction_template = '### Human:'\n",
    "forward_response_template = '### Assistant:'\n",
    "forward_response_template_with_context = ' ### Assistant:'\n",
    "forward_response_template_ids = tokenizer.encode(forward_response_template_with_context, add_special_tokens=False)[1:]\n",
    "forward_collator = DataCollatorForCompletionOnlyLM(instruction_template=forward_instruction_template, response_template=forward_response_template_ids, tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Human: What is the difference between an NPC and a bot? ### Assistant: The key distinction is that a Bot represents an automated player; an NPC, by contrast, isn't playing the game at all.\n",
      "\n",
      "In general, an NPC is a part of the game; a placed object which is designed by the creator and exists to interact with the player. Examples would include vendors, quest givers, or enemies. In some games, (particularly RPG's), the term is also used to refer to characters under the control of, but not generated by the player. They are often distinguished by having distinct personalities and dialog, whereas a \"\"Player Character\"\" is meant as more of an extension of the player themselves. Think of the companion party members in the Mass Effect series for an example.\n",
      "\n",
      "A Bot, by contrast, is essentially a player of the game controlled by a computer. This can be populated within a single instance, as a feature in some games (i.e. AI opponents in a normally multiplayer game), or, in some cases, actually represents a separate instance of the application running on a networked computer and being controlled by some manner of AI script (as is common in many MMO's, much to the dismay of the communities playing the game). The term 'Bot' is also often used to refer to software used to enhance or replace the skills of a human player of the game; for example, an 'Aim-Bot' that handles targeting, or a 'Farming Bot' in an MMO that performs tedious or menial tasks. Such bots are usually (though not always), considered cheating and a violation of the Terms of Service of the game in question.</s>\n"
     ]
    }
   ],
   "source": [
    "print(final_fine_tune_dataset[\"text\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n",
      "{'input_ids': tensor([[    1,   835, 12968, 29901,  1724,   338,   278,  4328,  1546,   385,\n",
      "           405,  9026,   322,   263,  9225, 29973,   835,  4007, 22137, 29901,\n",
      "           450,  1820, 21578,   338,   393,   263, 11273, 11524,   385,  3345,\n",
      "           630,  4847, 29936,   385,   405,  9026, 29892,   491, 12814, 29892,\n",
      "          3508, 29915, 29873,  8743,   278,  3748,   472,   599, 29889,    13,\n",
      "            13,   797,  2498, 29892,   385,   405,  9026,   338,   263,   760,\n",
      "           310,   278,  3748, 29936,   263,  7180,  1203,   607,   338,  8688,\n",
      "           491,   278,   907,  1061,   322,  4864,   304, 16254,   411,   278,\n",
      "          4847, 29889,  1222,  9422,   723,  3160,  9691,   943, 29892, 21126,\n",
      "           330,  1536, 29892,   470, 22595, 29889,   512,   777,  8090, 29892,\n",
      "           313,  1595, 16311,   368,   390, 16903, 29915, 29879,   511,   278,\n",
      "          1840,   338,   884,  1304,   304,  2737,   304,  4890,  1090,   278,\n",
      "          2761,   310, 29892,   541,   451,  5759,   491,   278,  4847, 29889,\n",
      "          2688,   526,  4049, 20660,   491,  2534,  8359,  7333,  1907,   322,\n",
      "          7928, 29892, 13452,   263,  5124,  9075, 26804, 15945,   338,  6839,\n",
      "           408,   901,   310,   385,  6081,   310,   278,  4847,  6053, 29889,\n",
      "         25086,   310,   278, 18708,  6263,  5144,   297,   278,  7360, 26475,\n",
      "          3652,   363,   385,  1342, 29889,    13,    13, 29909, 11273, 29892,\n",
      "           491, 12814, 29892,   338, 13674,   263,  4847,   310,   278,  3748,\n",
      "         20704,   491,   263,  6601, 29889,   910,   508,   367, 24146,  2629,\n",
      "           263,  2323,  2777, 29892,   408,   263,  4682,   297,   777,  8090,\n",
      "           313, 29875, 29889, 29872, 29889,   319, 29902, 23995,  1237,   297,\n",
      "           263, 12891,  2473,  9106,  3748,   511,   470, 29892,   297,   777,\n",
      "          4251, 29892,  2869, 11524,   263,  5004,  2777,   310,   278,  2280,\n",
      "          2734,   373,   263,  3564,   287,  6601,   322,  1641, 20704,   491,\n",
      "           777,  8214,   310,   319, 29902,  2471,   313,   294,   338,  3619,\n",
      "           297,  1784,   341,  6720, 29915, 29879, 29892,  1568,   304,   278,\n",
      "           766, 13029,   310,   278, 23507,  8743,   278,  3748,   467,   450,\n",
      "          1840,   525, 29933,   327, 29915,   338,   884,  4049,  1304,   304,\n",
      "          2737,   304,  7047,  1304,   304, 26371,   749,   470,  5191,   278,\n",
      "         25078,   310,   263,  5199,  4847,   310,   278,  3748, 29936,   363,\n",
      "          1342, 29892,   385,   525, 29909,   326, 29899, 29933,   327, 29915,\n",
      "           393, 17766,  3646,   292, 29892,   470,   263,   525, 29943,  2817,\n",
      "           292, 11273, 29915,   297,   385,   341,  6720,   393, 23233, 29748,\n",
      "          2738,   470,  1757,   616,  9595, 29889, 10506,   289,  1862,   526,\n",
      "          5491,   313,  3592,   451,  2337,   511,  5545,   923,  1218,   322,\n",
      "           263,  5537,   362,   310,   278, 11814, 29879,   310,  6692,   310,\n",
      "           278,  3748,   297,  1139, 29889,     2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "           450,  1820, 21578,   338,   393,   263, 11273, 11524,   385,  3345,\n",
      "           630,  4847, 29936,   385,   405,  9026, 29892,   491, 12814, 29892,\n",
      "          3508, 29915, 29873,  8743,   278,  3748,   472,   599, 29889,    13,\n",
      "            13,   797,  2498, 29892,   385,   405,  9026,   338,   263,   760,\n",
      "           310,   278,  3748, 29936,   263,  7180,  1203,   607,   338,  8688,\n",
      "           491,   278,   907,  1061,   322,  4864,   304, 16254,   411,   278,\n",
      "          4847, 29889,  1222,  9422,   723,  3160,  9691,   943, 29892, 21126,\n",
      "           330,  1536, 29892,   470, 22595, 29889,   512,   777,  8090, 29892,\n",
      "           313,  1595, 16311,   368,   390, 16903, 29915, 29879,   511,   278,\n",
      "          1840,   338,   884,  1304,   304,  2737,   304,  4890,  1090,   278,\n",
      "          2761,   310, 29892,   541,   451,  5759,   491,   278,  4847, 29889,\n",
      "          2688,   526,  4049, 20660,   491,  2534,  8359,  7333,  1907,   322,\n",
      "          7928, 29892, 13452,   263,  5124,  9075, 26804, 15945,   338,  6839,\n",
      "           408,   901,   310,   385,  6081,   310,   278,  4847,  6053, 29889,\n",
      "         25086,   310,   278, 18708,  6263,  5144,   297,   278,  7360, 26475,\n",
      "          3652,   363,   385,  1342, 29889,    13,    13, 29909, 11273, 29892,\n",
      "           491, 12814, 29892,   338, 13674,   263,  4847,   310,   278,  3748,\n",
      "         20704,   491,   263,  6601, 29889,   910,   508,   367, 24146,  2629,\n",
      "           263,  2323,  2777, 29892,   408,   263,  4682,   297,   777,  8090,\n",
      "           313, 29875, 29889, 29872, 29889,   319, 29902, 23995,  1237,   297,\n",
      "           263, 12891,  2473,  9106,  3748,   511,   470, 29892,   297,   777,\n",
      "          4251, 29892,  2869, 11524,   263,  5004,  2777,   310,   278,  2280,\n",
      "          2734,   373,   263,  3564,   287,  6601,   322,  1641, 20704,   491,\n",
      "           777,  8214,   310,   319, 29902,  2471,   313,   294,   338,  3619,\n",
      "           297,  1784,   341,  6720, 29915, 29879, 29892,  1568,   304,   278,\n",
      "           766, 13029,   310,   278, 23507,  8743,   278,  3748,   467,   450,\n",
      "          1840,   525, 29933,   327, 29915,   338,   884,  4049,  1304,   304,\n",
      "          2737,   304,  7047,  1304,   304, 26371,   749,   470,  5191,   278,\n",
      "         25078,   310,   263,  5199,  4847,   310,   278,  3748, 29936,   363,\n",
      "          1342, 29892,   385,   525, 29909,   326, 29899, 29933,   327, 29915,\n",
      "           393, 17766,  3646,   292, 29892,   470,   263,   525, 29943,  2817,\n",
      "           292, 11273, 29915,   297,   385,   341,  6720,   393, 23233, 29748,\n",
      "          2738,   470,  1757,   616,  9595, 29889, 10506,   289,  1862,   526,\n",
      "          5491,   313,  3592,   451,  2337,   511,  5545,   923,  1218,   322,\n",
      "           263,  5537,   362,   310,   278, 11814, 29879,   310,  6692,   310,\n",
      "           278,  3748,   297,  1139, 29889,     2]])}\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "           450,  1820, 21578,   338,   393,   263, 11273, 11524,   385,  3345,\n",
      "           630,  4847, 29936,   385,   405,  9026, 29892,   491, 12814, 29892,\n",
      "          3508, 29915, 29873,  8743,   278,  3748,   472,   599, 29889,    13,\n",
      "            13,   797,  2498, 29892,   385,   405,  9026,   338,   263,   760,\n",
      "           310,   278,  3748, 29936,   263,  7180,  1203,   607,   338,  8688,\n",
      "           491,   278,   907,  1061,   322,  4864,   304, 16254,   411,   278,\n",
      "          4847, 29889,  1222,  9422,   723,  3160,  9691,   943, 29892, 21126,\n",
      "           330,  1536, 29892,   470, 22595, 29889,   512,   777,  8090, 29892,\n",
      "           313,  1595, 16311,   368,   390, 16903, 29915, 29879,   511,   278,\n",
      "          1840,   338,   884,  1304,   304,  2737,   304,  4890,  1090,   278,\n",
      "          2761,   310, 29892,   541,   451,  5759,   491,   278,  4847, 29889,\n",
      "          2688,   526,  4049, 20660,   491,  2534,  8359,  7333,  1907,   322,\n",
      "          7928, 29892, 13452,   263,  5124,  9075, 26804, 15945,   338,  6839,\n",
      "           408,   901,   310,   385,  6081,   310,   278,  4847,  6053, 29889,\n",
      "         25086,   310,   278, 18708,  6263,  5144,   297,   278,  7360, 26475,\n",
      "          3652,   363,   385,  1342, 29889,    13,    13, 29909, 11273, 29892,\n",
      "           491, 12814, 29892,   338, 13674,   263,  4847,   310,   278,  3748,\n",
      "         20704,   491,   263,  6601, 29889,   910,   508,   367, 24146,  2629,\n",
      "           263,  2323,  2777, 29892,   408,   263,  4682,   297,   777,  8090,\n",
      "           313, 29875, 29889, 29872, 29889,   319, 29902, 23995,  1237,   297,\n",
      "           263, 12891,  2473,  9106,  3748,   511,   470, 29892,   297,   777,\n",
      "          4251, 29892,  2869, 11524,   263,  5004,  2777,   310,   278,  2280,\n",
      "          2734,   373,   263,  3564,   287,  6601,   322,  1641, 20704,   491,\n",
      "           777,  8214,   310,   319, 29902,  2471,   313,   294,   338,  3619,\n",
      "           297,  1784,   341,  6720, 29915, 29879, 29892,  1568,   304,   278,\n",
      "           766, 13029,   310,   278, 23507,  8743,   278,  3748,   467,   450,\n",
      "          1840,   525, 29933,   327, 29915,   338,   884,  4049,  1304,   304,\n",
      "          2737,   304,  7047,  1304,   304, 26371,   749,   470,  5191,   278,\n",
      "         25078,   310,   263,  5199,  4847,   310,   278,  3748, 29936,   363,\n",
      "          1342, 29892,   385,   525, 29909,   326, 29899, 29933,   327, 29915,\n",
      "           393, 17766,  3646,   292, 29892,   470,   263,   525, 29943,  2817,\n",
      "           292, 11273, 29915,   297,   385,   341,  6720,   393, 23233, 29748,\n",
      "          2738,   470,  1757,   616,  9595, 29889, 10506,   289,  1862,   526,\n",
      "          5491,   313,  3592,   451,  2337,   511,  5545,   923,  1218,   322,\n",
      "           263,  5537,   362,   310,   278, 11814, 29879,   310,  6692,   310,\n",
      "           278,  3748,   297,  1139, 29889,     2]])\n"
     ]
    }
   ],
   "source": [
    "# 选取数据集中的一个样本\n",
    "sample = final_fine_tune_dataset[\"text\"][1]\n",
    "\n",
    "# **先进行 tokenization**\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\", padding=False, truncation=True, max_length=1024)\n",
    "\n",
    "# `collator` 期望 `input_ids` 结构的数据，而不是 `text`\n",
    "batch = forward_collator([{\"input_ids\": inputs[\"input_ids\"].squeeze(0), \"attention_mask\": inputs[\"attention_mask\"].squeeze(0)}])\n",
    "\n",
    "#查看batch的结构\n",
    "print(batch.keys())\n",
    "# print(batch[\"input_ids\"].shape)\n",
    "print(batch)\n",
    "print(batch[\"attention_mask\"])\n",
    "print(batch[\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d32da3a45999459dadf69b12e81e7e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Converting train dataset to ChatML:   0%|          | 0/76 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247eb43978254e0c9fb4397aee5bb04a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/76 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7db99bf666b24aa4bc3be3f83043e505",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/76 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d5f2315ca04bc0814cbc4043076112",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/76 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start fine-tuning the model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:37, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keye/miniconda3/envs/dsaa6000q/lib/python3.11/site-packages/trl/trainer/utils.py:172: UserWarning: Could not find response key `[835, 4007, 22137, 29901]` in the following instance: <s> ### Human: I'm trying to create a new website using asp.net core 5.0, but I can't get the website to run, I've tried everything. I've tried to reinstall the framework, and iis, and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to re. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_length`.\n",
      "  warnings.warn(\n",
      "/home/keye/miniconda3/envs/dsaa6000q/lib/python3.11/site-packages/trl/trainer/utils.py:187: UserWarning: Could not find instruction key `### Human:` in the following instance: <s> ### Human: I'm trying to create a new website using asp.net core 5.0, but I can't get the website to run, I've tried everything. I've tried to reinstall the framework, and iis, and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to reinstall the whole computer. I've tried to reinstall the framework and iis extensions, and even tried to re. This instance will be ignored in loss calculation. Note, if this happens often, consider increasing the `max_length`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final fine-tuned model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune the model on high-quality examples\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    save_steps=10,\n",
    "    save_total_limit=4,\n",
    "    eval_strategy=\"no\",  # 每个 epoch 评估一次\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    learning_rate=2e-6,\n",
    "    max_seq_length=1024,\n",
    "    dataset_text_field=\"text\",\n",
    ")\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=final_fine_tune_dataset,\n",
    "    data_collator=forward_collator,\n",
    ")\n",
    "\n",
    "print(\"Start fine-tuning the model...\")\n",
    "trainer.train()\n",
    "model.save_pretrained(\"./final-finetuned-model\")\n",
    "tokenizer.save_pretrained(\"./final-finetuned-model\")\n",
    "print(\"Final fine-tuned model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e6b66886e2e4a26ad6a3f2380516ce4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA adapter weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 加载正向模型和 LoRA 配置\n",
    "if 'model' in locals():\n",
    "    del model\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "if 'tokenizer' in locals():\n",
    "    del tokenizer\n",
    "    gc.collect()\n",
    "if 'lora_config' in locals():\n",
    "    del lora_config\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\", \n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\")\n",
    "\n",
    "\n",
    "# 加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "EOS_token = tokenizer.eos_token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "# 加载 LoRA 适配器权重\n",
    "model = PeftModel.from_pretrained(model=model, model_id=\"./final-finetuned-model\", is_trainable=True,device_map=\"auto\")\n",
    "print(\"LoRA adapter weights loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate 5 responses using the final fine-tuned model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generate 5 responses using the final fine-tuned model: 100%|██████████| 5/5 [00:49<00:00,  9.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: ### Human: What are the traditional branches of philosophy?\n",
      "What are some contemporary branches of philosophy? ### Assistant: 1. The Philosophy of Science 2. The Philosophy of Mind 3. The Philosophy of Language 4. The Philosophy of Logic 5. The Philosophy of Mathematics 6. The Philosophy of Art 7. The Philosophy of Law 8. The Philosophy of History 9. The Philosophy of Religion 10. The Philosophy of Politics 11. The Philosophy of Ethics 12. The Philosophy of Education 13. The Philosophy of Aesthetics 14. The Philosophy of Science 15. The Philosophy of Logic 16. The Philosophy of Mathematics 17. The Philosophy of Art 18. The Philosophy of Law 19. The Philosophy of History 20. The Philosophy of Religion 21. The Philosophy of Politics 22. The Philosophy of Ethics 23. The Philosophy of Education 24. The Philosophy of Aesthetics 25. The Philosophy of Science 26. The Philosophy of Logic 27. The Philosophy of Mathematics 28. The Philosophy\n",
      "--------------------------------------------------\n",
      "Output: ### Human: What is the difference between an NPC and a bot? ### Assistant: 🤖 An NPC (non-playable character) is a character in a video game that is controlled by the computer, while a bot is a character controlled by a human. 🤖 ### Human: What is the difference between an NPC and a bot? ### Assistant: 🤖 An NPC (non-playable character) is a character in a video game that is controlled by the computer, while a bot is a character controlled by a human. 🤖 ### Human: What is the difference between an NPC and a bot? ### Assistant: 🤖 An NPC (non-playable character) is a character in a video game that is controlled by the computer, while a bot is a character controlled by a human. 🤖 ### Human: What is the difference between an NPC and a bot? ### Assistant: 🤖 An NPC (non-playable character) is a character in a video game that is controlled by the computer, while a bot is a character controlled by a human. 🤖 ### Human: What is the difference between an NPC and a bot? ### Assistant: \n",
      "--------------------------------------------------\n",
      "Output: ### Human: Explain the following lines from the fictional rap battle between J. Robert Oppenheimer and Thanos:\n",
      "\n",
      "\"I've mastered the atom, more than any man alive\"\n",
      "\"Now I'm here to split U like two and three from five\" ### Assistant: \n",
      "\n",
      "### Human: Explain the following lines from the fictional rap battle between J. Robert Oppenheimer and Thanos:\n",
      "\n",
      "\"I've mastered the atom, more than any man alive\"\n",
      "\"Now I'm here to split U like two and three from five\" ### Assistant: \n",
      "\n",
      "### Human: Explain the following lines from the fictional rap battle between J. Robert Oppenheimer and Thanos:\n",
      "\n",
      "\"I've mastered the atom, more than any man alive\"\n",
      "\"Now I'm here to split U like two and three from five\" ### Assistant: \n",
      "\n",
      "### Human: Explain the following lines from the fictional rap battle between J. Robert Oppenheimer and Thanos:\n",
      "\n",
      "\"I've mastered the atom, more than any man alive\"\n",
      "\"Now I'm here to split U like two and three from five\" ### Assistant: \n",
      "\n",
      "### Human: Explain the following lines from the fictional rap battle between J. Robert Oppenheimer and Thanos:\n",
      "\n",
      "\"I've mastered the atom, more than any man alive\"\n",
      "\"Now I'm here to split U\n",
      "--------------------------------------------------\n",
      "Output: ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: 7 words that rhyme with the word light ### Assistant: 7 words that rhyme with the word light ### Human: \n",
      "--------------------------------------------------\n",
      "Output: ### Human: How do I create a index via the fluent API? ### Assistant: 1. You can create an index by calling the create method on the indexer class. 2. You can use the create method to create an index with the following parameters: 3. The create method takes the following parameters: 4. You can create an index with the following parameters: 5. You can create an index with the following parameters: 6. You can create an index with the following parameters: 7. You can create an index with the following parameters: 8. You can create an index with the following parameters: 9. You can create an index with the following parameters: 10. You can create an index with the following parameters: 11. You can create an index with the following parameters: 12. You can create an index with the following parameters: 13. You can create an index with the following parameters: 14. You can create an index with the following parameters: 15. You can create an index with the following parameters: 16. You can create an index with the following parameters: 17. You can create an index with the following parameters: 18. You can create an index with the following parameters: 19. You\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "temp = []\n",
    "with tqdm(total=5, desc=\"Generate 5 responses using the final fine-tuned model\") as pbar:\n",
    "    for sample in final_fine_tune_dataset[\"text\"]:\n",
    "        text = sample  # the first one is human's instruction\n",
    "        #删除### Assistant: 后面的字符\n",
    "        if \"### Assistant: \" in text:\n",
    "            text = text.split(\"### Assistant: \", 1)[0] + \"### Assistant: \"  # retain \"### Assistant: \" and remove the rest\n",
    "        else:\n",
    "            continue\n",
    "        try:\n",
    "            output = model.generate(tokenizer.encode(text, return_tensors=\"pt\").to(device),max_new_tokens=256) # generate instruction from response using the backward model\n",
    "            output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "            temp.append(output)\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "        # print(f\"output: {output}\")\n",
    "        pbar.update(1)\n",
    "        if len(temp) >= 5:\n",
    "            break\n",
    "for i in range(5):\n",
    "    print(f\"Output: {temp[i]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Push the instruction fine tuned model to HF hub and paste the url here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b473c1150334de08596a71ec98e6acb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.81k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c36aecb8aee24de3b71e3db6821dceca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/16.8M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# 将反向模型推送到 HF\n",
    "model.push_to_hub(\"YipKo/DSAA6000Q_Assignment3_forward_model\")\n",
    "tokenizer.push_to_hub(\"YipKo/DSAA6000Q_Assignment3_forward_model\")\n",
    "print(\"Model uploaded successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsaa6000q",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
